Key scripts for creating a Neural Network;

- `TrainSequnceNNetV3.m`: Trains a neural network. Performs both forward and back propogation in the loop. [See Overview for more details](#overview)
  - ``Takes a Neural network model, dataset, 
  - `InputMask`: array the size of a single observation in input data, used to mask steps in the RNN sequence.
  - `OutputMask`: array the size of a single observation in output data, used to mask steps in the prediction RNN sequence, before calculating the loss gradient.
  - `NNModels`: a sequence of NNLayer objects generated by `GenerateNNetLayer.m`. Combined they form a multi-layer neural network.
  - `trainingSettings`: optimization settings
  - `epochs`: number of epochs
  - `varargin`: Training settings;
    - 1: inference mode
    - 2: displays training progress
    - 3: Data validation (NOT IMPLEMENTED)
- `NNPropogate.m`: Performs propogation through the layer of a neural network. Supports LSTM, Attentional LSTM, Bi-Directional LSTM, Teacher forcing, sequence masking.
  - `NNLayer`: Network layer created by `GenerateNNetLayer.m`
  - `DataIN`: observation to propogate through the layer.
  - `propdirection`: "forward", or "backward"
  - `fcn_types`: mapping of activation types to activation function index found in `NNLayer` (for memory efficiency)
  - `varargin`: additional parameters (depending on `propdirection`)
- `LayerStatePCAcheck.m`: Plots Principal Components of the hidden states and/or LSTM cell states. (For introspection on training progress)

See `LSTMExamples.m` for implementation of a LSTM Neural network creation and training.

## Overview

Main algorithm **TrainSequnceNNetV3.m** accepts a combination of layers which are RNNs or normal fully connected layers, and infers how they are connected by looking at the settings defined when the individual layers are created.
Supported features;
- gradient clipping
- learning rules: SGD, SGD w Learning rate schedule, Momentum, Adam, ~~Adagrad~~, Adadelta
- Cost functions: Multiclass & Binary Cross-entropy, L2, L1, Weighted binary cross entropy ([use case](https://samoliverschumacher.github.io/website/post/eeg_costtradeoff/#custom-loss-function---recurrent-neural-network-lstm))
- Automatic weight initialiser selector (based on activation function)
- LSTM cell with peephole connections
- input sequence masking
- teacher forcing
- Attention mechanism for attentional encoder-decoders
- BiDirectional LSTM

## W.I.P.

To be added;
- Stacked LSTM support
- batch normalisation
- parralel computing support
- Other types of NNet layers: GRU, Convoutional, bayesian neural network!

The TrainSequenceNNetV3.m allows any combinnation of fully connected & LSTM layers to be input, it can (should) figure out how to connect them together. i.e. FC -> LSTM -> LSTM -> FC or LSTM -> FC, or LSTM -> LSTM. 
Will try to add examples of how it can be flexible to different configurations, and how to do so.

See the repository machinelearningprojects/toyProjects/lstmRomannumerals for a working example. 

This is purely a project for teaching and learning, it's been my education in LSTMs. Please feel free to use & improve!
FYI - if you just want to use a LSTM in matlab, there are toolboxes that do a much much better job!
